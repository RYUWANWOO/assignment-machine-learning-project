{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_08_solution_latest.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rgD-FF4MA9Kg",
        "SnBT9fWXA9Kj"
      ],
      "machine_shape": "hm"
    },
    "interpreter": {
      "hash": "84bbda367bac7e7bffd9b7890a44d65326aaedad40e5a9021c2651157391b1ef"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQGQXmOpA9J-"
      },
      "source": [
        "# Unsupervised image denoising"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiKuXIvrYCNj"
      },
      "source": [
        "## Connect Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK5OQ94EXkHs",
        "outputId": "7edd1e33-3e01-4a42-8011-6de9a75a3632"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGBcGR2LA9KI"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNpI2DZ6A9KJ"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from torchvision.transforms import Compose, ToTensor, ToPILImage, Resize, Lambda, Normalize, Grayscale\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from math import log10\n",
        "from tqdm.notebook import tqdm\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU32jTFuA9KM"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf-s-8nrA9KN"
      },
      "source": [
        "directory_data  = 'drive/MyDrive'\n",
        "filename_data   = 'assignment_08_data.npz'\n",
        "data            = np.load(os.path.join(directory_data, filename_data))\n",
        "\n",
        "\n",
        "original_train = data['original_train'] \n",
        "noise_train    = data['noise_train']\n",
        "\n",
        "original_test = data['original_test'] \n",
        "noise_test    = data['noise_test']\n",
        "\n",
        "print('*************************************************')\n",
        "print('size of noise_train    : ', noise_train.shape)\n",
        "print('*************************************************')\n",
        "print('size of noise_test    : ', noise_test.shape)\n",
        "print('*************************************************')\n",
        "print('number of training image :', noise_train.shape[0])\n",
        "print('height of training image :', noise_train.shape[1])\n",
        "print('width of training image  :', noise_train.shape[2])\n",
        "print('*************************************************')\n",
        "print('number of testing image :', noise_test.shape[0])\n",
        "print('height of testing image :', noise_test.shape[1])\n",
        "print('width of testing image  :', noise_test.shape[2])\n",
        "print('*************************************************')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20TiZsA_A9KO"
      },
      "source": [
        "## Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtDynD4nA9KP"
      },
      "source": [
        "device        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ==================================================\n",
        "# determine optimal hyper-parameters to obtain best testing performance\n",
        "number_epoch    = 100\n",
        "size_minibatch  = 32\n",
        "learning_rate   = 0.1\n",
        "momentum        = 0.9\n",
        "weight_decay    = 0.0001\n",
        "weight_total_variation = 0.0001\n",
        "# =================================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AbHCZPpA9KR"
      },
      "source": [
        "## Costumize dataloader for pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UveCjrBA9KS"
      },
      "source": [
        "class dataset (Dataset):\n",
        "  def  __init__(self, original,noise):\n",
        "\n",
        "    self.original = original\n",
        "    self.noise    = noise\n",
        "        \n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    original    = self.original[index]\n",
        "    noise       = self.noise[index]\n",
        "    \n",
        "    original   = torch.FloatTensor(original).unsqueeze(dim=0)\n",
        "    noise      = torch.FloatTensor(noise).unsqueeze(dim=0)\n",
        "\n",
        "\n",
        "    return (original , noise)\n",
        "  \n",
        "  def __len__(self):\n",
        "\n",
        "     return self.original.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "357alwWgA9KU"
      },
      "source": [
        "## Construct datasets and dataloaders for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7JDqjSVA9KU"
      },
      "source": [
        "dataset_train = dataset(original_train, noise_train) \n",
        "dataset_test  = dataset(original_test, noise_test) \n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=size_minibatch, shuffle=True, drop_last=True, num_workers=2)\n",
        "dataloader_test  = DataLoader(dataset_test,  batch_size=size_minibatch, shuffle=False, drop_last=True, num_workers=2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DQ3_o-VA9KV"
      },
      "source": [
        "## Shape of the data with data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCNL_T-HA9KW"
      },
      "source": [
        "(original_train, noise_train)   = dataset_train[0]\n",
        "(original_test, noise_test)     = dataset_test[0]\n",
        "\n",
        "print('************************************************************')\n",
        "print('shape of the original image in the training dataset:', original_train.shape)\n",
        "print('shape of the noisy image in the training dataset:', noise_train.shape)\n",
        "print('************************************************************')\n",
        "print('shape of the original image in the testing dataset:', original_test.shape)\n",
        "print('shape of the noisy image in the testing dataset:', noise_test.shape)\n",
        "print('************************************************************')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yn9jMLrA9KX"
      },
      "source": [
        "## Class for the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esyw9M--A9KY"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network,self).__init__()\n",
        "\n",
        "        # -------------------------------------------------\n",
        "        # Encoder\n",
        "        # -------------------------------------------------\n",
        "        self.e_layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, bias=True),  \n",
        "                        nn.BatchNorm2d(16),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(0.2),\n",
        "                        nn.MaxPool2d(2,2),\n",
        "\n",
        "        )\n",
        "        self.e_layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True), \n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(0.2),\n",
        "                        nn.MaxPool2d(2,2),\n",
        "        )\n",
        "        # -------------------------------------------------\n",
        "        # Decoder\n",
        "        # -------------------------------------------------\n",
        "        \n",
        "        self.d_layer1 = nn.Sequential(\n",
        "                        #nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                        nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "                        nn.BatchNorm2d(16),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(0.2),\n",
        "\n",
        "        )\n",
        "         \n",
        "        self.d_layer2 = nn.Sequential(\n",
        "                        #nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                        nn.Conv2d(in_channels=16, out_channels=1, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "                        nn.Sigmoid(),\n",
        "                        nn.Dropout(0.2),\n",
        "        )\n",
        "\n",
        "        \n",
        "        # -------------------------------------------------\n",
        "        # Network\n",
        "        # -------------------------------------------------\n",
        "        self.network = nn.Sequential(\n",
        "                        self.e_layer1,\n",
        "                        self.e_layer2,\n",
        "                        self.d_layer1,\n",
        "                        self.d_layer2,\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "    \n",
        "        out = self.network(x)\n",
        "      \n",
        "        return out\n",
        "\n",
        "    # ======================================================================\n",
        "    # initialize weights\n",
        "    # ======================================================================\n",
        "    def initialize_weight(self):\n",
        "            \n",
        "        for m in self.network.modules():\n",
        "            \n",
        "            if isinstance(m, nn.Conv2d):\n",
        "\n",
        "                nn.init.xavier_uniform_(m.weight) \n",
        "                \n",
        "                if m.bias is not None:\n",
        "\n",
        "                    nn.init.constant_(m.bias, 0.01)\n",
        "                    pass\n",
        "                    \n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                \n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                # nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0.01)\n",
        "                \n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                # nn.init.constant_(m.weight, 1) \n",
        "\n",
        "                if m.bias is not None:\n",
        "                    \n",
        "                    nn.init.constant_(m.bias, 0.01)\n",
        "                    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTptbDGZA9KZ"
      },
      "source": [
        "## Build the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MgUbMJdA9KZ"
      },
      "source": [
        "model = Network().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum , weight_decay=weight_decay)\n",
        "#optimizer = torch.optim.Adam(model.parameters(),lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,lr_lambda=lambda epoch: 0.95**epoch,last_epoch=-1,verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAMmqhrUA9KZ"
      },
      "source": [
        "## Compute prediction (denoised image)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sE58VDWA9Ka"
      },
      "source": [
        "def compute_prediction(input, model):\n",
        "    \n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    prediction = model(input)\n",
        "    # ==================================================\n",
        "    \n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4MU2uUwA9Kc"
      },
      "source": [
        "## Compute loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzLZ5_ijB13L"
      },
      "source": [
        "def compute_fidelity(input, prediction):\n",
        "    \n",
        "    mse = nn.MSELoss()\n",
        "    \n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    loss_mse = mse(input,prediction)\n",
        "    # ==================================================\n",
        "    \n",
        "    loss_mse_value = loss_mse.item() \n",
        "    \n",
        "    return loss_mse, loss_mse_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J9-SmZeDDHW"
      },
      "source": [
        "def compute_regularization(data, weight):\n",
        "    \n",
        "    bs_img, c_img, h_img, w_img = data.size()\n",
        "   \n",
        "    tv_height = torch.abs(data[:, :, 1:, :] - data[:, :, :- 1, :]).sum()\n",
        "    tv_width  = torch.abs(data[:, :, :, 1:] - data[:, :, :, :-1]).sum()\n",
        "\n",
        "    total_variation = (tv_height + tv_width) / (bs_img * c_img * h_img * w_img)\n",
        "    \n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    loss_regularization = total_variation * weight\n",
        "    # ==================================================\n",
        "    \n",
        "    loss_regularization_value = loss_regularization.item()\n",
        "     \n",
        "    return loss_regularization, loss_regularization_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grQ11S2uA9Kc"
      },
      "source": [
        "def compute_loss(input, prediction, weight):\n",
        "\n",
        "    (loss_fidelity, loss_fidelity_value) = compute_fidelity(input, prediction)\n",
        "    (loss_regularization, loss_regularization_value) = compute_regularization(prediction, weight) \n",
        "\n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    loss = loss_fidelity + loss_regularization\n",
        "    # ==================================================\n",
        "    \n",
        "    loss_value = loss.item()\n",
        "    \n",
        "    return loss, loss_value , loss_fidelity_value, loss_regularization_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1EWYXG6A9Kc"
      },
      "source": [
        "## Compute PSNR metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtVX7nLCA9Kd"
      },
      "source": [
        "def compute_PSNR(mse):\n",
        "\n",
        "    if (mse==0.):\n",
        "        PSNR=100    \n",
        "    else :\n",
        "        PSNR=10*log10(1.0 / mse)\n",
        "        \n",
        "    return PSNR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfi3toLTA9Kd"
      },
      "source": [
        "## Variable for the learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re9vSZWLA9Kd"
      },
      "source": [
        "loss_fidelity_mean_train       = np.zeros(number_epoch)\n",
        "loss_fidelity_std_train        = np.zeros(number_epoch)\n",
        "loss_regularization_mean_train = np.zeros(number_epoch)\n",
        "loss_regularization_std_train  = np.zeros(number_epoch)\n",
        "loss_mean_train                = np.zeros(number_epoch)\n",
        "loss_std_train                 = np.zeros(number_epoch)\n",
        "PSNR_mean_train                = np.zeros(number_epoch)\n",
        "PSNR_std_train                 = np.zeros(number_epoch)\n",
        "\n",
        "loss_fidelity_mean_test       = np.zeros(number_epoch)\n",
        "loss_fidelity_std_test        = np.zeros(number_epoch)\n",
        "loss_regularization_mean_test = np.zeros(number_epoch)\n",
        "loss_regularization_std_test  = np.zeros(number_epoch)\n",
        "loss_mean_test                = np.zeros(number_epoch)\n",
        "loss_std_test                 = np.zeros(number_epoch)\n",
        "PSNR_mean_test                = np.zeros(number_epoch)\n",
        "PSNR_std_test                 = np.zeros(number_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2DK1BgcA9Ke"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z6RHIxcA9Ke"
      },
      "source": [
        "def train(model, dataloader):\n",
        "\n",
        "    loss_epoch          = []\n",
        "    loss_fidelity_epoch = []\n",
        "    loss_reg_epoch      = []\n",
        "    psnr_epoch          = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for index_batch, (original, noise) in enumerate(dataloader):\n",
        "\n",
        "        original = original.to(device)\n",
        "        noise    = noise.to(device)\n",
        "        \n",
        "        prediction = compute_prediction(noise, model)\n",
        "        (loss, loss_value, loss_fidelity_value, loss_regularization_value) = compute_loss(noise, prediction, weight_total_variation)\n",
        "        \n",
        "        # ==================================================\n",
        "        # fill up the blank\n",
        "        (mse, mse_value)    = compute_fidelity(prediction,original)\n",
        "        psnr                = compute_PSNR(mse_value)\n",
        "        # ==================================================\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_epoch.append(loss_value)\n",
        "        loss_fidelity_epoch.append(loss_fidelity_value)\n",
        "        loss_reg_epoch.append(loss_regularization_value)\n",
        "        psnr_epoch.append(psnr)\n",
        "\n",
        "    loss_mean_epoch     = np.mean(loss_epoch)\n",
        "    loss_std_epoch      = np.std(loss_epoch)\n",
        "    \n",
        "    loss_fidelity_mean_epoch     = np.mean(loss_fidelity_epoch)\n",
        "    loss_fidelity_std_epoch      = np.std(loss_fidelity_epoch)\n",
        "    \n",
        "    loss_reg_mean_epoch     = np.mean(loss_reg_epoch)\n",
        "    loss_reg_std_epoch      = np.std(loss_reg_epoch)\n",
        "\n",
        "    psnr_mean_epoch = np.mean(psnr_epoch)\n",
        "    psnr_std_epoch  = np.std(psnr_epoch)\n",
        "\n",
        "    loss                       = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
        "    loss_fidelity              = {'mean' : loss_fidelity_mean_epoch, 'std' : loss_fidelity_std_epoch}\n",
        "    loss_regularization        = {'mean' : loss_reg_mean_epoch, 'std' : loss_reg_std_epoch}\n",
        "    psnr                       = {'mean' : psnr_mean_epoch, 'std' : psnr_std_epoch}\n",
        "\n",
        "    return (loss, loss_fidelity, loss_regularization, psnr)    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z76W6RQWA9Ke"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9FO3jlwA9Ke"
      },
      "source": [
        "def test(model, dataloader):\n",
        "\n",
        "    loss_epoch          = []\n",
        "    loss_fidelity_epoch = []\n",
        "    loss_reg_epoch      = []\n",
        "    psnr_epoch          = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for index_batch, (original, noise) in enumerate(dataloader):\n",
        "\n",
        "        original = original.to(device)\n",
        "        noise    = noise.to(device)\n",
        "\n",
        "        prediction = compute_prediction(noise, model)\n",
        "        (loss, loss_value, loss_fidelity_value, loss_regularization_value) = compute_loss(noise, prediction, weight_total_variation)\n",
        "        \n",
        "        # ==================================================\n",
        "        # fill up the blank\n",
        "        (mse, mse_value)    = compute_fidelity(prediction,original)\n",
        "        psnr                = compute_PSNR(mse_value)\n",
        "        # ==================================================\n",
        "\n",
        "        loss_epoch.append(loss_value)\n",
        "        loss_fidelity_epoch.append(loss_fidelity_value)\n",
        "        loss_reg_epoch.append(loss_regularization_value)\n",
        "        psnr_epoch.append(psnr)\n",
        "\n",
        "    loss_mean_epoch     = np.mean(loss_epoch)\n",
        "    loss_std_epoch      = np.std(loss_epoch)\n",
        "    \n",
        "    loss_fidelity_mean_epoch     = np.mean(loss_fidelity_epoch)\n",
        "    loss_fidelity_std_epoch      = np.std(loss_fidelity_epoch)\n",
        "    \n",
        "    loss_reg_mean_epoch     = np.mean(loss_reg_epoch)\n",
        "    loss_reg_std_epoch      = np.std(loss_reg_epoch)\n",
        "\n",
        "    psnr_mean_epoch = np.mean(psnr_epoch)\n",
        "    psnr_std_epoch  = np.std(psnr_epoch)\n",
        "\n",
        "    loss                       = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
        "    loss_fidelity              = {'mean' : loss_fidelity_mean_epoch, 'std' : loss_fidelity_std_epoch}\n",
        "    loss_regularization        = {'mean' : loss_reg_mean_epoch, 'std' : loss_reg_std_epoch}\n",
        "    psnr                       = {'mean' : psnr_mean_epoch, 'std' : psnr_std_epoch}\n",
        "\n",
        "    return (loss, loss_fidelity, loss_regularization, psnr)    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl_p_5B2A9Kf"
      },
      "source": [
        "## train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kpLyy6bA9Kf"
      },
      "source": [
        "# ================================================================================\n",
        "# \n",
        "# iterations for epochs\n",
        "#\n",
        "# ================================================================================\n",
        "for i in tqdm(range(number_epoch)):\n",
        "    \n",
        "    # ================================================================================\n",
        "    # \n",
        "    # training\n",
        "    #\n",
        "    # ================================================================================\n",
        "    (loss_train, loss_fidelity_train, loss_reg_train, psnr_train) = train(model, dataloader_train)\n",
        "\n",
        "    loss_mean_train[i] = loss_train['mean']\n",
        "    loss_std_train[i]  = loss_train['std']\n",
        "    \n",
        "    loss_fidelity_mean_train[i] = loss_fidelity_train['mean']\n",
        "    loss_fidelity_std_train[i]  = loss_fidelity_train['std']\n",
        "    \n",
        "    loss_regularization_mean_train[i] = loss_reg_train['mean']\n",
        "    loss_regularization_std_train[i]  = loss_reg_train['std']\n",
        "\n",
        "    PSNR_mean_train[i]  = psnr_train['mean']\n",
        "    PSNR_std_train[i]   = psnr_train['std']\n",
        "\n",
        "    # ================================================================================\n",
        "    # \n",
        "    # testing\n",
        "    #\n",
        "    # ================================================================================\n",
        "    (loss_test, loss_fidelity_test, loss_reg_test, psnr_test) = test(model, dataloader_test)\n",
        "\n",
        "    loss_mean_test[i] = loss_test['mean']\n",
        "    loss_std_test[i]  = loss_test['std']\n",
        "\n",
        "    loss_fidelity_mean_test[i] = loss_fidelity_test['mean']\n",
        "    loss_fidelity_std_test[i]  = loss_fidelity_test['std']\n",
        "    \n",
        "    loss_regularization_mean_test[i] = loss_reg_test['mean']\n",
        "    loss_regularization_std_test[i]  = loss_reg_test['std']\n",
        "\n",
        "    PSNR_mean_test[i]  = psnr_test['mean']\n",
        "    PSNR_std_test[i]   = psnr_test['std']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxek0ypkA9Kf"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4N32do7A9Kg"
      },
      "source": [
        "# functions for visualizing the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY1ul6ttA9Kg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgD-FF4MA9Kg"
      },
      "source": [
        "## Plot functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7SzyQnaA9Kg"
      },
      "source": [
        "def plot_data_grid(data, index_data, nRow, nCol):\n",
        "    \n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * 3, nRow * 3))\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVYA6nBGA9Kh"
      },
      "source": [
        "def plot_data_tensor_grid(data, index_data, nRow, nCol):\n",
        "    \n",
        "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * 3, nRow * 3))\n",
        "\n",
        "    data = data.detach().cpu().squeeze(axis=1)\n",
        "\n",
        "    for i in range(nRow):\n",
        "        for j in range(nCol):\n",
        "\n",
        "            k       = i * nCol + j\n",
        "            index   = index_data[k]\n",
        "\n",
        "            axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[i, j].xaxis.set_visible(False)\n",
        "            axes[i, j].yaxis.set_visible(False)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r06Lf6UNA9Kh"
      },
      "source": [
        "def plot_curve_error(data_mean, data_std, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.title(title)\n",
        "\n",
        "    alpha = 0.3\n",
        "    \n",
        "    plt.plot(range(len(data_mean)), data_mean, '-', color = 'red')\n",
        "    plt.fill_between(range(len(data_mean)), data_mean - data_std, data_mean + data_std, facecolor = 'blue', alpha = alpha) \n",
        "    \n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBmjFM9tA9Kh"
      },
      "source": [
        "def print_curve(data, index):\n",
        "    \n",
        "    for i in range(len(index)):\n",
        "\n",
        "        idx = index[i]\n",
        "        val = data[idx]\n",
        "\n",
        "        print('index = %2d, value = %12.10f' % (idx, val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5_ePFORA9Ki"
      },
      "source": [
        "def get_data_last(data, index_start):\n",
        "\n",
        "    data_last = data[index_start:]\n",
        "\n",
        "    return data_last"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFNAv5nnA9Ki"
      },
      "source": [
        "def get_max_last_range(data, index_start):\n",
        "\n",
        "    data_range = get_data_last(data, index_start)\n",
        "    value = data_range.max()\n",
        "\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0FhjnneA9Ki"
      },
      "source": [
        "def get_min_last_range(data, index_start):\n",
        "\n",
        "    data_range = get_data_last(data, index_start)\n",
        "    value = data_range.min()\n",
        "\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR2U5WquA9Kj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBT9fWXA9Kj"
      },
      "source": [
        "# functions for presenting the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak9mEQ11A9Kj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mSysHDcA9Kk"
      },
      "source": [
        "def function_result_01():\n",
        "\n",
        "    print('[plot examples of the training images]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 5 \n",
        "    nCol = 4\n",
        "    index_data  = np.arange(0, nRow * nCol)\n",
        "    image_train = dataset_train.noise[index_data]\n",
        "\n",
        "    plot_data_grid(image_train, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG5Dv-BtA9Kk"
      },
      "source": [
        "def function_result_02():\n",
        "\n",
        "    print('[plot examples of the training denoising results]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 5\n",
        "    nCol = 4\n",
        "    index_data          = np.arange(0, nRow * nCol)\n",
        "    image_train         = torch.FloatTensor(dataset_train.noise[index_data]).unsqueeze(dim=1).to(device)\n",
        "    prediction_train    = compute_prediction(image_train,model)\n",
        "    \n",
        "    plot_data_tensor_grid(prediction_train, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AOFrS4wA9Kk"
      },
      "source": [
        "def function_result_03():\n",
        "\n",
        "    print('[plot examples of the testing images]')\n",
        "    print('') \n",
        "    \n",
        "    nRow = 5\n",
        "    nCol = 4\n",
        "    index_data = np.arange(0, nRow * nCol)\n",
        "    image_test = dataset_test.noise[index_data]\n",
        "\n",
        "    plot_data_grid(image_test, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2jAuO-HA9Kl"
      },
      "source": [
        "def function_result_04():\n",
        "\n",
        "    print('[plot examples of the testing denoising results]')\n",
        "    print('') \n",
        "\n",
        "    nRow = 5\n",
        "    nCol = 4\n",
        "    index_data      = np.arange(0, nRow * nCol)\n",
        "    image_test      = torch.FloatTensor(dataset_test.noise[index_data]).unsqueeze(dim=1).to(device)\n",
        "    prediction_test = compute_prediction(image_test,model)\n",
        "\n",
        "    plot_data_tensor_grid(prediction_test, index_data, nRow, nCol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q_8MOVKA9Kl"
      },
      "source": [
        "def function_result_05():\n",
        "\n",
        "    print('[plot the training loss]')\n",
        "    print('') \n",
        "\n",
        "    plot_curve_error(loss_mean_train, loss_std_train, 'epoch', 'loss', 'training loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4aH0BefQhUi"
      },
      "source": [
        "def function_result_06():\n",
        "\n",
        "    print('[plot the training fidelity loss]')\n",
        "    print('') \n",
        "\n",
        "    plot_curve_error(loss_fidelity_mean_train, loss_fidelity_std_train, 'epoch', 'loss', 'training loss (fidelity)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHnNBNcQw9a"
      },
      "source": [
        "def function_result_07():\n",
        "\n",
        "    print('[plot the training regularization loss]')\n",
        "    print('') \n",
        "\n",
        "    plot_curve_error(loss_regularization_mean_train, loss_regularization_std_train, 'epoch', 'loss', 'training loss (regularization)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvRfH6ejA9Kl"
      },
      "source": [
        "def function_result_08():\n",
        "\n",
        "    print('[plot the training PSNR]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(PSNR_mean_train, PSNR_std_train, 'epoch', 'PSNR', 'training PSNR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08ILy67PA9Km"
      },
      "source": [
        "def function_result_09():\n",
        "    \n",
        "    print('[plot the testing loss]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(loss_mean_test, loss_std_test, 'epoch', 'loss', 'testing loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzRElUYcREsr"
      },
      "source": [
        "def function_result_10():\n",
        "    \n",
        "    print('[plot the testing fidelity loss]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(loss_fidelity_mean_test, loss_fidelity_std_test, 'epoch', 'loss', 'testing loss (fidelity)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp0_wwYyRM8f"
      },
      "source": [
        "def function_result_11():\n",
        "    \n",
        "    print('[plot the testing regularization loss]')\n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(loss_regularization_mean_test, loss_regularization_std_test, 'epoch', 'loss', 'testing loss (regularization)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHDnt4ndA9Km"
      },
      "source": [
        "def function_result_12():\n",
        "    \n",
        "    print('[plot the testing PSNR]') \n",
        "    print('') \n",
        "    \n",
        "    plot_curve_error(PSNR_mean_test, PSNR_std_test, 'epoch', 'PSNR', 'testing PSNR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HlkP764A9Km"
      },
      "source": [
        "def function_result_13():\n",
        "    \n",
        "    print('[print the training loss (mean) at the last 10 epochs]')\n",
        "    print('') \n",
        "\n",
        "    data_last = get_data_last(loss_mean_train, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JAxHIggA9Kn"
      },
      "source": [
        "def function_result_14():\n",
        "    \n",
        "    print('[print the training PSNR (mean) at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last = get_data_last(PSNR_mean_train, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHP4nAgxA9Kn"
      },
      "source": [
        "def function_result_15():\n",
        "    \n",
        "    print('[print the testing loss (mean) at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last = get_data_last(loss_mean_test, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lslaY-A1A9Ko"
      },
      "source": [
        "def function_result_16():\n",
        "    \n",
        "    print('[print the testing PSNR (mean) at the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    data_last = get_data_last(PSNR_mean_test, -10)\n",
        "    index = np.arange(0, 10)\n",
        "    print_curve(data_last, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_xYLoXkA9Ko"
      },
      "source": [
        "def function_result_17():\n",
        "    \n",
        "    print('[print the best training PSNR (mean) within the last 10 epochs]')\n",
        "    print('') \n",
        "\n",
        "    value = get_max_last_range(PSNR_mean_train, -10)\n",
        "    print('best training PSNR = %12.10f' % (value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d0Zf7lsA9Ko"
      },
      "source": [
        "def function_result_18():\n",
        "    \n",
        "    print('[print the best testing PSNR (mean) within the last 10 epochs]')\n",
        "    print('') \n",
        "    \n",
        "    value = get_max_last_range(PSNR_mean_test, -10)\n",
        "    print('best testing PSNR = %12.10f' % (value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-Nw6A2jA9Kp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa6-Ktz8A9Kp"
      },
      "source": [
        "# RESULTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHREmjnvA9Kp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cGBA-ccA9Kp"
      },
      "source": [
        "number_result = 18\n",
        "\n",
        "for i in range(number_result):\n",
        "\n",
        "    title           = '# RESULT # {:02d}'.format(i+1) \n",
        "    name_function   = 'function_result_{:02d}()'.format(i+1)\n",
        "\n",
        "    print('') \n",
        "    print('################################################################################')\n",
        "    print('#') \n",
        "    print(title)\n",
        "    print('#') \n",
        "    print('################################################################################')\n",
        "    print('') \n",
        "\n",
        "    eval(name_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2JnRWygXHL4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}